{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the documentation of the Vectorian, a fast search engine based on word embeddings and customizable alignments. If you are new here, start with the Introduction .","title":"Home"},{"location":"documents/","text":"Documents What is a Document? A Document in Vectorian terminology is some text we can perform a search on, e.g. book, a web page, or a Shakespeare play. For example, the text of Shakespeare's A Midsummer Night's Dream and various metadata (such as speakers, acts and scenes) about it could be regarded as one Document . Partitions and Spans A problem in this terminology arises when we want to search a text on a finer granularity level, e.g. when we aim to find all of the Document 's sentences that match some query. For example, let us assume we want to find all sentences in A Midsummer Night's Dream that contain a mention of Titania . If we regard the whole text of A Midsummer Night's Dream as one single Document , then the result of our search would be one single hit, telling us that A Midsummer Night's Dream indeed contains one occurence of Titania . This obviously is neither very useful nor not what we set out to achieve. A common approach to solving this problem is to change terminology and split the Document into its sentences beforehand and then regard those sentences as separate Document s. This approach can be confusing though and has a number of technical and conceptual drawbacks. The Vectorian takes a different approach and introduces a concept called Partition , which is a combination of granularity control (the Vectorian API calls this the Partition s level) and a sliding window specification. Search operations do not operate on Document s, but instead on Span s that are generated through applying a Partition on a Document . A Span is a section of a Document, e.g. a single sentence or a specific sequence of tokens. Here a three examples to illustrate the concepts - we assume that doc is a Document and that session is a Session : doc.spans(session.partition(\"document\")) produces one Span that represents the document as a whole. doc.spans(doc.partition(\"sentence\")) produces Span s such that each Span is one sentence of the Document . doc.spans(doc.partition(\"token\", 5, 1)) produces Span by applying a sliding window of size 5 to the tokens of the Document . Which granularity levels (e.g. \"document\" or \"sentence\" ) are available depends on the Importer used (see next section). The default Importer s support \"document\" , \"sentence\" and \"token\" . Importers An Importer converts some input (e.g. plain text, a XML file, a Web URL, ...) into a Document . It will perform tokenization, partition detection (e.g. detection of sentence boundaries) and other related nlp tasks like lemmatization. The Importer is also responsible for computing contextual embeddings. Standard importers in the Vectorian include: StringImporter : turns a string into a Document TextImporter : turns a plain text file into a Document NovelImporter : imports a novel provided as a plain text file and keeps additional data about the structure of the novel (e.g. chapter breaks) in the resulting Document PlayShakespeareImporter : imports data from an XML file from https://www.playshakespeare.com/ - the resulting Document knows about speakers, acts and scenes for each line Here is an example of using StringImporter to create a Document (you need a spaCy nlp object that knows about the used language): im = vectorian.importers.StringImporter(nlp) doc = im(\"At the station is an art museum.\") Importer s - once created - act like factories for Document s, i.e. they can be used to import multiple Document s. Persistence Since the task of importing Document s can take considerable time - due to various nlp pipeline steps and/or the computation of contextual embeddings - it is useful to save and load imported Document s to and from disk by using the Document 's save and the static Document.load methods. Note that these methods do not take a filename, but rather a path, since a single Document might consist of a number of files. For saving and loading multiple Document 's, the Corpus class provides some convenience functionality. First, construct a Corpus from your imported Document 's. Then use Corpus.save (and later Corpus.load ). As with Document 's, these methods take a path (not a single filename). You can also construct a corpus by incrementally saving corpus sub sets to a path and then later on loading the whole Corpus in one go. Be aware that saving the same set of Documents to the same path will currently cause duplication.","title":"Documents"},{"location":"documents/#documents","text":"","title":"Documents"},{"location":"documents/#what-is-a-document","text":"A Document in Vectorian terminology is some text we can perform a search on, e.g. book, a web page, or a Shakespeare play. For example, the text of Shakespeare's A Midsummer Night's Dream and various metadata (such as speakers, acts and scenes) about it could be regarded as one Document .","title":"What is a Document?"},{"location":"documents/#partitions-and-spans","text":"A problem in this terminology arises when we want to search a text on a finer granularity level, e.g. when we aim to find all of the Document 's sentences that match some query. For example, let us assume we want to find all sentences in A Midsummer Night's Dream that contain a mention of Titania . If we regard the whole text of A Midsummer Night's Dream as one single Document , then the result of our search would be one single hit, telling us that A Midsummer Night's Dream indeed contains one occurence of Titania . This obviously is neither very useful nor not what we set out to achieve. A common approach to solving this problem is to change terminology and split the Document into its sentences beforehand and then regard those sentences as separate Document s. This approach can be confusing though and has a number of technical and conceptual drawbacks. The Vectorian takes a different approach and introduces a concept called Partition , which is a combination of granularity control (the Vectorian API calls this the Partition s level) and a sliding window specification. Search operations do not operate on Document s, but instead on Span s that are generated through applying a Partition on a Document . A Span is a section of a Document, e.g. a single sentence or a specific sequence of tokens. Here a three examples to illustrate the concepts - we assume that doc is a Document and that session is a Session : doc.spans(session.partition(\"document\")) produces one Span that represents the document as a whole. doc.spans(doc.partition(\"sentence\")) produces Span s such that each Span is one sentence of the Document . doc.spans(doc.partition(\"token\", 5, 1)) produces Span by applying a sliding window of size 5 to the tokens of the Document . Which granularity levels (e.g. \"document\" or \"sentence\" ) are available depends on the Importer used (see next section). The default Importer s support \"document\" , \"sentence\" and \"token\" .","title":"Partitions and Spans"},{"location":"documents/#importers","text":"An Importer converts some input (e.g. plain text, a XML file, a Web URL, ...) into a Document . It will perform tokenization, partition detection (e.g. detection of sentence boundaries) and other related nlp tasks like lemmatization. The Importer is also responsible for computing contextual embeddings. Standard importers in the Vectorian include: StringImporter : turns a string into a Document TextImporter : turns a plain text file into a Document NovelImporter : imports a novel provided as a plain text file and keeps additional data about the structure of the novel (e.g. chapter breaks) in the resulting Document PlayShakespeareImporter : imports data from an XML file from https://www.playshakespeare.com/ - the resulting Document knows about speakers, acts and scenes for each line Here is an example of using StringImporter to create a Document (you need a spaCy nlp object that knows about the used language): im = vectorian.importers.StringImporter(nlp) doc = im(\"At the station is an art museum.\") Importer s - once created - act like factories for Document s, i.e. they can be used to import multiple Document s.","title":"Importers"},{"location":"documents/#persistence","text":"Since the task of importing Document s can take considerable time - due to various nlp pipeline steps and/or the computation of contextual embeddings - it is useful to save and load imported Document s to and from disk by using the Document 's save and the static Document.load methods. Note that these methods do not take a filename, but rather a path, since a single Document might consist of a number of files. For saving and loading multiple Document 's, the Corpus class provides some convenience functionality. First, construct a Corpus from your imported Document 's. Then use Corpus.save (and later Corpus.load ). As with Document 's, these methods take a path (not a single filename). You can also construct a corpus by incrementally saving corpus sub sets to a path and then later on loading the whole Corpus in one go. Be aware that saving the same set of Documents to the same path will currently cause duplication.","title":"Persistence"},{"location":"embeddings/","text":"Embeddings Token Embeddings vs. Document Embeddings Note that the term embedding in this whole section always refers to token embeddings . Document or sentence embeddings are not handled via the Embedding class in the Vectorian. In order to use the latter, use a SpanEmbeddingSimilarity together with an Index , see the sections on Span Similarity and Index . Overview Here is an overview of the different classes the API offers to load and specify embeddings: The following sections will explain the different kinds of classes in detail. The Zoo One of the easiest way to create an Embedding instance is to use the Zoo class, which offers a small set of pretrained embeddings. Note that Vectorian's model zoo is not aimed at offering a wide variety of currently available embeddings and is more geared towards enabling quick and easy experimentation within memory-constrained environments like Binder by offering compressed versions of common standard embeddings. To get a list of all available models, call vectorian.embeddings.Zoo.list() To load one embedding by its name, e.g. fasttext-en-mini use: fasttext_mini = Zoo.load('fasttext-en-mini') Static Embeddings Static Embeddings are embeddings that map one token string to one embedding vector, independent of the token's occurence in a text. All implementations derive from StaticEmbedding . The Vectorian supports two kinds of flavors: keyed values (e.g. Word2vec, GloVe, Numberbatch) variants of fastText Whereas the first variant is a simple key-value mapping, the structure of fastText embeddings is more complex (and powerful) due to its n-gram representation. A second criterion when loading embeddings is whether you use a standard pretrained embedding from an external provider (such as Facebook), or whether you want to load embeddings you trained yourself. In the first case, the Vectorian API takes care of downloading and caching the embedding data in order to make it easy to use for new users. Keyed Values Pretrained keyed values embeddings can be loaded either via Vectorian's Zoo or via PretrainedGensimVectors , if the embedding is available through gensim-data (see https://radimrehurek.com/gensim/downloader.html). For GloVe, there is also the PretrainedGloVe class. To load any other keyed values embedding, use the Word2VecVectors class. It takes a unique name and a path to a Word2vec formatted file. Note that the contents of that file need not originate from Word2vec and can contain any key-value-form embedding such as GloVe or Numberbatch. fastText PretrainedFastText offers easy access to the official pretrained fastText embbeddings offered by Facebook (see https://fasttext.cc/). Simply specify the desired language code in the constructor and Vectorian will download the necessary data. Note that these downloads are large and will have memory requirements that are beyond what is provided by Binder. CompressedFastTextVectors allows the loading of fastText data that has been compressed via https://github.com/avidale/compress-fasttext. This approach can reduce the memory requirements of fastText by several magnitudes without a huge loss in quality. Sampling A small but important topic with static topic is how embedding vectors are to be chosen given various tokens that have been normalized to the same base form. The approach taken is refered to as sampling in the Vectorian and can be configured in various Embedding classes (see the embedding_sampling argument in the PretrainedGensimVectors constructor for example). Let us assume we have two tokens, \"behold\" and \"Behold\", that have both been normalized to the same form \"behold\" through a lowercase-rule in the Session 's normalizers settings (see Session ). The Vectorian offers two options how to deal with this: By default (using the \"nearest\" setting for sampling ), the Vectorian will choose the embedding of the nearest key, namely \"behold\", to look up the vector for both tokens. Thus, both \"behold\" and \"Behold\" will get the embedding vector that is given for \"behold\" in the embedding data. If specifying the \"average\" setting for sampling , the Vectorian will look up the vectors for both \"behold\" and \"Behold\", then average these, and then apply this averaged vector to both \"behold\" and \"Behold\". Note that sampling will not be an issue if no token text normalization is specified in the Session (i.e. if there are no unified tokens). Contextual Embeddings Difference from Static Embeddings In contrast to static embeddings, contextual embeddings provide every token instance in a text with a potentially different embedding vector. For example, in \"to be or not to be\", the tokens \"to\" and \"be\" might get different embeddings depending on their occurrence in the phrase. Thus, the bold \"to\" in \" to be or not to be\" might be different from the bold \"to\" in \"to be or not to be\". Static embeddings on the other hand would map \"to\" to exactly one (static) vector all the time. Contextual embeddings are often generated from network architectures such as ELMO, BERT and various newer Transformer-based architectures. Usage in the Vectorian To use contextual embeddings in the Vectorian, there are two options that provide access to embeddings computed through a spaCy pipeline: SpacyVectorEmbedding uses the vector attribute in spaCy's Token class to obtain embedding vectors. This works well with dedicated packages auch as the spaCy Sentence-BERT wrapper (see https://pypi.org/project/spacy-sentence-bert/). SpacyTransformerEmbedding obtains an embedding vector by access spaCy's internal Transformer state (when using a Transformer model such as en_core_web_trf ). The second option is highly experimental and has a number of shortcomings (for example, it is debatable if the embeddings acquired in this way are even suitable as contextual embeddings since there is no control over the Transformer layers which contribute to them). In general, the first option is to be preferred. Compression It can be useful to compress contextual embeddings, since they can take up a large amount of disk space (this can also impact search performance). Note that compression is always a tradeoff between size and quality. To obtain a PCA-compressed version of a contextual embedding, use the compressed method inside the embedding's class. The Vectorian will then automatically take care of compressing queries that run on those embeddings in the correct way. Stacking Embeddings A common technique to combine the benefits of different existing embeddings into one new embedding is to stack them (i.e. appending their vectors). In the Vectorian, this can be achieved by using the StackedEmbedding class and providing the embeddings you want to stack. At the time of this writing, this is only supported for static embeddings.","title":"Embeddings"},{"location":"embeddings/#embeddings","text":"","title":"Embeddings"},{"location":"embeddings/#token-embeddings-vs-document-embeddings","text":"Note that the term embedding in this whole section always refers to token embeddings . Document or sentence embeddings are not handled via the Embedding class in the Vectorian. In order to use the latter, use a SpanEmbeddingSimilarity together with an Index , see the sections on Span Similarity and Index .","title":"Token Embeddings vs. Document Embeddings"},{"location":"embeddings/#overview","text":"Here is an overview of the different classes the API offers to load and specify embeddings: The following sections will explain the different kinds of classes in detail.","title":"Overview"},{"location":"embeddings/#the-zoo","text":"One of the easiest way to create an Embedding instance is to use the Zoo class, which offers a small set of pretrained embeddings. Note that Vectorian's model zoo is not aimed at offering a wide variety of currently available embeddings and is more geared towards enabling quick and easy experimentation within memory-constrained environments like Binder by offering compressed versions of common standard embeddings. To get a list of all available models, call vectorian.embeddings.Zoo.list() To load one embedding by its name, e.g. fasttext-en-mini use: fasttext_mini = Zoo.load('fasttext-en-mini')","title":"The Zoo"},{"location":"embeddings/#static-embeddings","text":"Static Embeddings are embeddings that map one token string to one embedding vector, independent of the token's occurence in a text. All implementations derive from StaticEmbedding . The Vectorian supports two kinds of flavors: keyed values (e.g. Word2vec, GloVe, Numberbatch) variants of fastText Whereas the first variant is a simple key-value mapping, the structure of fastText embeddings is more complex (and powerful) due to its n-gram representation. A second criterion when loading embeddings is whether you use a standard pretrained embedding from an external provider (such as Facebook), or whether you want to load embeddings you trained yourself. In the first case, the Vectorian API takes care of downloading and caching the embedding data in order to make it easy to use for new users.","title":"Static Embeddings"},{"location":"embeddings/#keyed-values","text":"Pretrained keyed values embeddings can be loaded either via Vectorian's Zoo or via PretrainedGensimVectors , if the embedding is available through gensim-data (see https://radimrehurek.com/gensim/downloader.html). For GloVe, there is also the PretrainedGloVe class. To load any other keyed values embedding, use the Word2VecVectors class. It takes a unique name and a path to a Word2vec formatted file. Note that the contents of that file need not originate from Word2vec and can contain any key-value-form embedding such as GloVe or Numberbatch.","title":"Keyed Values"},{"location":"embeddings/#fasttext","text":"PretrainedFastText offers easy access to the official pretrained fastText embbeddings offered by Facebook (see https://fasttext.cc/). Simply specify the desired language code in the constructor and Vectorian will download the necessary data. Note that these downloads are large and will have memory requirements that are beyond what is provided by Binder. CompressedFastTextVectors allows the loading of fastText data that has been compressed via https://github.com/avidale/compress-fasttext. This approach can reduce the memory requirements of fastText by several magnitudes without a huge loss in quality.","title":"fastText"},{"location":"embeddings/#sampling","text":"A small but important topic with static topic is how embedding vectors are to be chosen given various tokens that have been normalized to the same base form. The approach taken is refered to as sampling in the Vectorian and can be configured in various Embedding classes (see the embedding_sampling argument in the PretrainedGensimVectors constructor for example). Let us assume we have two tokens, \"behold\" and \"Behold\", that have both been normalized to the same form \"behold\" through a lowercase-rule in the Session 's normalizers settings (see Session ). The Vectorian offers two options how to deal with this: By default (using the \"nearest\" setting for sampling ), the Vectorian will choose the embedding of the nearest key, namely \"behold\", to look up the vector for both tokens. Thus, both \"behold\" and \"Behold\" will get the embedding vector that is given for \"behold\" in the embedding data. If specifying the \"average\" setting for sampling , the Vectorian will look up the vectors for both \"behold\" and \"Behold\", then average these, and then apply this averaged vector to both \"behold\" and \"Behold\". Note that sampling will not be an issue if no token text normalization is specified in the Session (i.e. if there are no unified tokens).","title":"Sampling"},{"location":"embeddings/#contextual-embeddings","text":"","title":"Contextual Embeddings"},{"location":"embeddings/#difference-from-static-embeddings","text":"In contrast to static embeddings, contextual embeddings provide every token instance in a text with a potentially different embedding vector. For example, in \"to be or not to be\", the tokens \"to\" and \"be\" might get different embeddings depending on their occurrence in the phrase. Thus, the bold \"to\" in \" to be or not to be\" might be different from the bold \"to\" in \"to be or not to be\". Static embeddings on the other hand would map \"to\" to exactly one (static) vector all the time. Contextual embeddings are often generated from network architectures such as ELMO, BERT and various newer Transformer-based architectures.","title":"Difference from Static Embeddings"},{"location":"embeddings/#usage-in-the-vectorian","text":"To use contextual embeddings in the Vectorian, there are two options that provide access to embeddings computed through a spaCy pipeline: SpacyVectorEmbedding uses the vector attribute in spaCy's Token class to obtain embedding vectors. This works well with dedicated packages auch as the spaCy Sentence-BERT wrapper (see https://pypi.org/project/spacy-sentence-bert/). SpacyTransformerEmbedding obtains an embedding vector by access spaCy's internal Transformer state (when using a Transformer model such as en_core_web_trf ). The second option is highly experimental and has a number of shortcomings (for example, it is debatable if the embeddings acquired in this way are even suitable as contextual embeddings since there is no control over the Transformer layers which contribute to them). In general, the first option is to be preferred.","title":"Usage in the Vectorian"},{"location":"embeddings/#compression","text":"It can be useful to compress contextual embeddings, since they can take up a large amount of disk space (this can also impact search performance). Note that compression is always a tradeoff between size and quality. To obtain a PCA-compressed version of a contextual embedding, use the compressed method inside the embedding's class. The Vectorian will then automatically take care of compressing queries that run on those embeddings in the correct way.","title":"Compression"},{"location":"embeddings/#stacking-embeddings","text":"A common technique to combine the benefits of different existing embeddings into one new embedding is to stack them (i.e. appending their vectors). In the Vectorian, this can be achieved by using the StackedEmbedding class and providing the embeddings you want to stack. At the time of this writing, this is only supported for static embeddings.","title":"Stacking Embeddings"},{"location":"introduction/","text":"Introduction Overview The following (simplified) diagram shows an overview of the most important classes and concepts of the Vectorian that you need to understand in order to perform searches over collections of texts. We will go through individual concepts in more detail later, but let us first get an understanding of how different concepts relate. In most abstract terms, to perform a search in the Vectorian you need to (refer to the diagram above): create a Session from Embedding s and Document s create an Index from the Session through specifying a Partition perform a search on the Index to get Result s and Match es Creating a Session create one or multiple Embedding s you want to work with. This boils down to choosing which embedding(s) you want to base your token similarity computations on. See Embeddings for more details. import (or load) one or multiple Document s that contain the text you want to search over. Document s in the Vectorian are created using different kinds of Importer s (these perform parsing tasks using a spaCy NLP class). See Documents for more details. create a Session that bundles Embedding s and Document s in a way that allows searches. See Sessions for more details. Creating an Index from your Session , create a Partition - which specifies how Document s should be split into searchable units, e.g. sentences. See Documents for more details. create a SpanSimilarity that models the approach to compare document spans. See Span Similarity for more details. from Partition and SpanSimilarity , create an Index - which specifies the strategy, i.e. algorithm, we employ for searching. See Index for more details. perform a search on that Index (using a query text) retrieve the Result and the Match es for that search Code Example We now present a series of concrete code examples that illustrate the steps described above. Prepare by importing the vectorian module: import vectorian Now create an Embedding . We choose the 50-dimensional variant of the pretrained GloVe 6B (see https://nlp.stanford.edu/projects/glove/): glove = vectorian.embeddings.PretrainedGloVe('6B', ndims=50) To turn an English text string into a Document , we first need an English text parser. From this, we can create a StringImporter : import spacy nlp = spacy.load(\"en_core_web_sm\") im = vectorian.importers.StringImporter(nlp) We can now create a Document from a string: doc = im(\"\"\" He'll go along o'er the wide world with me; Leave me alone to woo him. Let's away, And get our jewels and our wealth together, Devise the fittest time and safest way To hide us from pursuit that will be made After my flight. Now go we in content To liberty and not to banishment. \"\"\") # Celia in \"As You Like It\" The next piece of code creates a LabSession that brings together the Document s we want to search over ( doc in our case) and the embeddings we want to employ. vectorian.session.LabSession which is a special kind of Session suited for running inside Jupyter. If not running inside Jupyter, you might want to use vectorian.session.Session .` session = vectorian.session.LabSession( [doc], embeddings=[glove], normalizers=\"default\") Now we create a SpanFlowSimilarity , which is a special form of SpanSimilarity that allows us to specify the Waterman-Smith-Beyer alignment algorithm. Note that we base this alignment on a concept of token similarity that is computed through the the cosine similarity over the GloVe embedding we specified earlier. token_similarity = vectorian.similarity.TokenSimilarity( glove, vectorian.similarity.CosineSimilarity()) span_similarity = vectorian.similarity.SpanFlowSimilarity( token_sim=token_similarity, flow_strategy=vectorian.alignment.WatermanSmithBeyer( gap=vectorian.alignment.ExponentialGapCost(5), zero=0.25)) By creating Partition on a \"sentence\" level we instruct the Vectorian to split Document s into sentences and search over each of these. partition = session.partition(\"sentence\") From that sentence partition of the session's document corpus, we now create an Index that is based on the model of span and token similarity we created earlier through span_similarity . index = partition.index(span_similarity, nlp) We are now ready to submit a query. Let us look for one occurence of \"jewelry and riches\": result = index.find(\"jewelry and riches\", n=1) result If you are inside Jupyter, you will see the following result: If not in Jupyter, you can get a JSON summary of the same result data by calling: result.matches[0].to_json() This gives: {'slice': 2, 'location': {'start': 16, 'end': 27}, 'score': 0.8001667857170105, 'metric': 'glove-6B-50-cosine', 'regions': [{'s': \"alone to woo him. Let's away,\\nAnd get our \", 'gap_penalty': 0.0}, {'s': 'jewels', 'pos_s': 'NOUN', 'edges': [{'t': {'text': 'jewelry', 'index': 0, 'pos': 'NOUN'}, 'flow': 1.0, 'distance': 0.20577645301818848, 'metric': 'glove-6B-50-cosine'}]}, {'s': 'and', 'pos_s': 'CCONJ', 'edges': [{'t': {'text': 'and', 'index': 1, 'pos': 'CCONJ'}, 'flow': 1.0, 'distance': 0.0, 'metric': 'glove-6B-50-cosine'}]}, {'s': 'our ', 'gap_penalty': 0.12944944202899933}, {'s': 'wealth', 'pos_s': 'NOUN', 'edges': [{'t': {'text': 'riches', 'index': 2, 'pos': 'NOUN'}, 'flow': 1.0, 'distance': 0.26427364349365234, 'metric': 'glove-6B-50-cosine'}]}, {'s': 'together,\\nDevise the fittest time and safest way\\nTo hide ', 'gap_penalty': 0.0}], 'omitted': [], 'level': 'word'} Interactive Mode If calling the Python API seems too tedious and you just want to explore the options the Vectorian offers, the Vectorian offers an interactive GUI when run inside Jupyter. This GUI allows to configure a large part of what can be achieved through the Python API. It can therefore serve as a starting point for exploring available options and getting a better grasp of how different concepts relate to each other. If session is a LabSession and nlp is a spaCy instance, simply call session.interact(nlp) inside Jupyter. This will create an interactive search widget that allows you to perform searches.","title":"Introduction"},{"location":"introduction/#introduction","text":"","title":"Introduction"},{"location":"introduction/#overview","text":"The following (simplified) diagram shows an overview of the most important classes and concepts of the Vectorian that you need to understand in order to perform searches over collections of texts. We will go through individual concepts in more detail later, but let us first get an understanding of how different concepts relate. In most abstract terms, to perform a search in the Vectorian you need to (refer to the diagram above): create a Session from Embedding s and Document s create an Index from the Session through specifying a Partition perform a search on the Index to get Result s and Match es","title":"Overview"},{"location":"introduction/#creating-a-session","text":"create one or multiple Embedding s you want to work with. This boils down to choosing which embedding(s) you want to base your token similarity computations on. See Embeddings for more details. import (or load) one or multiple Document s that contain the text you want to search over. Document s in the Vectorian are created using different kinds of Importer s (these perform parsing tasks using a spaCy NLP class). See Documents for more details. create a Session that bundles Embedding s and Document s in a way that allows searches. See Sessions for more details.","title":"Creating a Session"},{"location":"introduction/#creating-an-index","text":"from your Session , create a Partition - which specifies how Document s should be split into searchable units, e.g. sentences. See Documents for more details. create a SpanSimilarity that models the approach to compare document spans. See Span Similarity for more details. from Partition and SpanSimilarity , create an Index - which specifies the strategy, i.e. algorithm, we employ for searching. See Index for more details. perform a search on that Index (using a query text) retrieve the Result and the Match es for that search","title":"Creating an Index"},{"location":"introduction/#code-example","text":"We now present a series of concrete code examples that illustrate the steps described above. Prepare by importing the vectorian module: import vectorian Now create an Embedding . We choose the 50-dimensional variant of the pretrained GloVe 6B (see https://nlp.stanford.edu/projects/glove/): glove = vectorian.embeddings.PretrainedGloVe('6B', ndims=50) To turn an English text string into a Document , we first need an English text parser. From this, we can create a StringImporter : import spacy nlp = spacy.load(\"en_core_web_sm\") im = vectorian.importers.StringImporter(nlp) We can now create a Document from a string: doc = im(\"\"\" He'll go along o'er the wide world with me; Leave me alone to woo him. Let's away, And get our jewels and our wealth together, Devise the fittest time and safest way To hide us from pursuit that will be made After my flight. Now go we in content To liberty and not to banishment. \"\"\") # Celia in \"As You Like It\" The next piece of code creates a LabSession that brings together the Document s we want to search over ( doc in our case) and the embeddings we want to employ. vectorian.session.LabSession which is a special kind of Session suited for running inside Jupyter. If not running inside Jupyter, you might want to use vectorian.session.Session .` session = vectorian.session.LabSession( [doc], embeddings=[glove], normalizers=\"default\") Now we create a SpanFlowSimilarity , which is a special form of SpanSimilarity that allows us to specify the Waterman-Smith-Beyer alignment algorithm. Note that we base this alignment on a concept of token similarity that is computed through the the cosine similarity over the GloVe embedding we specified earlier. token_similarity = vectorian.similarity.TokenSimilarity( glove, vectorian.similarity.CosineSimilarity()) span_similarity = vectorian.similarity.SpanFlowSimilarity( token_sim=token_similarity, flow_strategy=vectorian.alignment.WatermanSmithBeyer( gap=vectorian.alignment.ExponentialGapCost(5), zero=0.25)) By creating Partition on a \"sentence\" level we instruct the Vectorian to split Document s into sentences and search over each of these. partition = session.partition(\"sentence\") From that sentence partition of the session's document corpus, we now create an Index that is based on the model of span and token similarity we created earlier through span_similarity . index = partition.index(span_similarity, nlp) We are now ready to submit a query. Let us look for one occurence of \"jewelry and riches\": result = index.find(\"jewelry and riches\", n=1) result If you are inside Jupyter, you will see the following result: If not in Jupyter, you can get a JSON summary of the same result data by calling: result.matches[0].to_json() This gives: {'slice': 2, 'location': {'start': 16, 'end': 27}, 'score': 0.8001667857170105, 'metric': 'glove-6B-50-cosine', 'regions': [{'s': \"alone to woo him. Let's away,\\nAnd get our \", 'gap_penalty': 0.0}, {'s': 'jewels', 'pos_s': 'NOUN', 'edges': [{'t': {'text': 'jewelry', 'index': 0, 'pos': 'NOUN'}, 'flow': 1.0, 'distance': 0.20577645301818848, 'metric': 'glove-6B-50-cosine'}]}, {'s': 'and', 'pos_s': 'CCONJ', 'edges': [{'t': {'text': 'and', 'index': 1, 'pos': 'CCONJ'}, 'flow': 1.0, 'distance': 0.0, 'metric': 'glove-6B-50-cosine'}]}, {'s': 'our ', 'gap_penalty': 0.12944944202899933}, {'s': 'wealth', 'pos_s': 'NOUN', 'edges': [{'t': {'text': 'riches', 'index': 2, 'pos': 'NOUN'}, 'flow': 1.0, 'distance': 0.26427364349365234, 'metric': 'glove-6B-50-cosine'}]}, {'s': 'together,\\nDevise the fittest time and safest way\\nTo hide ', 'gap_penalty': 0.0}], 'omitted': [], 'level': 'word'}","title":"Code Example"},{"location":"introduction/#interactive-mode","text":"If calling the Python API seems too tedious and you just want to explore the options the Vectorian offers, the Vectorian offers an interactive GUI when run inside Jupyter. This GUI allows to configure a large part of what can be achieved through the Python API. It can therefore serve as a starting point for exploring available options and getting a better grasp of how different concepts relate to each other. If session is a LabSession and nlp is a spaCy instance, simply call session.interact(nlp) inside Jupyter. This will create an interactive search widget that allows you to perform searches.","title":"Interactive Mode"},{"location":"session/","text":"Session What is a Session? A Session brings together a fixed set of Document s and a fixed set of Embedding s. It is the basis for creating an Index . Creating a Session performs two important preprocessing steps of the data: all text and embedding data is mapped to a fixed vocabulary tokens are normalized according to given rules Establishing a fixed vocabulary (from Document s and Embedding s) allows the Vectorian to build data structures that are highly optimized for ensuing operations. More details on token normalization are given in the following section. Token and Tag Normalization A small but important parameter during Session creation is the normalizers option (which is usually set to \"default\"). normalizing tokens on a string level, e.g. lowercasing all tokens ignoring certain tokens from all subsequent operations unifying or mapping token POS tags It is in these settings that users declare whether two tokens like \"the\" and \"The\" should be regarded identical or not. If they are to be unified, the selection of embedding vectors depends on the configured embedding's sampling setting (see Embeddings ). The Default Normalization The Vectorian's default normalization is applies two sets of operations. On the text level: all non-word characters are removed from tokens (e.g. \"has-\" becomes \"has\") if tokens do not contain at least one letter, they are ignored On the POS tag level: tokens with POS tag \"PROPN\" are mapped to POS tag \"NOUN\" tokens with POS tag \"PUNCT\" (i.e. punctuation) are ignored The motivation for rewriting \"PROPN\" tags is that these often pose a problem for tag-weighted alignments due to their rather high inaccuracy. LabSession LabSession is a specialization of Session that is specifically geared towards use in Jupyter . It offers the following advantages over Session : displays a progress bar widget during performing queries Result s know how to render themselves in Jupyter as HTML","title":"Session"},{"location":"session/#session","text":"","title":"Session"},{"location":"session/#what-is-a-session","text":"A Session brings together a fixed set of Document s and a fixed set of Embedding s. It is the basis for creating an Index . Creating a Session performs two important preprocessing steps of the data: all text and embedding data is mapped to a fixed vocabulary tokens are normalized according to given rules Establishing a fixed vocabulary (from Document s and Embedding s) allows the Vectorian to build data structures that are highly optimized for ensuing operations. More details on token normalization are given in the following section.","title":"What is a Session?"},{"location":"session/#token-and-tag-normalization","text":"A small but important parameter during Session creation is the normalizers option (which is usually set to \"default\"). normalizing tokens on a string level, e.g. lowercasing all tokens ignoring certain tokens from all subsequent operations unifying or mapping token POS tags It is in these settings that users declare whether two tokens like \"the\" and \"The\" should be regarded identical or not. If they are to be unified, the selection of embedding vectors depends on the configured embedding's sampling setting (see Embeddings ).","title":"Token and Tag Normalization"},{"location":"session/#the-default-normalization","text":"The Vectorian's default normalization is applies two sets of operations. On the text level: all non-word characters are removed from tokens (e.g. \"has-\" becomes \"has\") if tokens do not contain at least one letter, they are ignored On the POS tag level: tokens with POS tag \"PROPN\" are mapped to POS tag \"NOUN\" tokens with POS tag \"PUNCT\" (i.e. punctuation) are ignored The motivation for rewriting \"PROPN\" tags is that these often pose a problem for tag-weighted alignments due to their rather high inaccuracy.","title":"The Default Normalization"},{"location":"session/#labsession","text":"LabSession is a specialization of Session that is specifically geared towards use in Jupyter . It offers the following advantages over Session : displays a progress bar widget during performing queries Result s know how to render themselves in Jupyter as HTML","title":"LabSession"},{"location":"sim_span/","text":"Span Similarity SpanSimilarity Classes that derive from SpanSimilarity model strategies to compare spans of tokens, e.g. sentences. In contrast to comparing pairs of single tokens - a concept that is called TokenSimilarity in the Vectorian API - SpanSimilarity describes how to compute a similarity score between two spans (i.e. sequences) of tokens, e.g. between to sentences. There are two kind of SpanSimilarity strategies the Vectorian supports, and they are handled in two separate classes: SpanFlowSimilarity models span flows (e.g. alignments and WMD) SpanEmbeddingSimilarity models span embeddings The term \"span flows\" encompasses anything that produces a flow or mapping between pairs of spans on a token level. On the one hand this can mean classical alignments - e.g. Needleman-Wunsch or Smith-Waterman. On the other hand, \"flows\" also include network-based approaches such as Word Mover's Distance. The exact kind of strategy is specified using a SpanFlowStrategy in the SpanFlowSimilarity constructor. The term \"span embeddings\" is a generalization of what is usually called \"document embeddings\" or \"sentence embeddings\". With this strategy we compute one embedding for a span of tokens. We do not specify an explicit strategy for dealing with individual tokens. Instead we provide an encoder that takes a span of tokens and outputs an embedding. To allow various kinds of approaches (such as building span embeddings from token embeddings) the actual encoder is wrapped inside other classes such as SpanEncoder and PartitionEncoder (more on this later). Here is an example of setting up a Waterman-Smith-Beyer alignment that uses cosine similarity over a pretrained GloVe embedding of dimension 50: vectorian.metrics.SpanFlowSimilarity( token_sim=vectorian.metrics( vectorian.embeddings.PretrainedGloVe('6B', ndims=50), vectorian.metrics.CosineSimilarity() ), flow_strategy=vectorian.alignment.WatermanSmithBeyer( gap=vectorian.alignment.ExponentialGapCost(cutoff=5), zero=0.25)) SpanFlowStrategy SpanFlowStrategy models a strategy to compute a network flow on the bipartite graph that models a pair of token spans (with tokens as nodes and each part of the graph modelling one span). Specific strategies to do this include classical alignments (e.g. Needleman-Wunsch) and variants of the Word Mover's Distance. The following diagram shows the current implementations for SpanFlowStrategy available in the Vectorian.","title":"Span Similarity"},{"location":"sim_span/#span-similarity","text":"","title":"Span Similarity"},{"location":"sim_span/#spansimilarity","text":"Classes that derive from SpanSimilarity model strategies to compare spans of tokens, e.g. sentences. In contrast to comparing pairs of single tokens - a concept that is called TokenSimilarity in the Vectorian API - SpanSimilarity describes how to compute a similarity score between two spans (i.e. sequences) of tokens, e.g. between to sentences. There are two kind of SpanSimilarity strategies the Vectorian supports, and they are handled in two separate classes: SpanFlowSimilarity models span flows (e.g. alignments and WMD) SpanEmbeddingSimilarity models span embeddings The term \"span flows\" encompasses anything that produces a flow or mapping between pairs of spans on a token level. On the one hand this can mean classical alignments - e.g. Needleman-Wunsch or Smith-Waterman. On the other hand, \"flows\" also include network-based approaches such as Word Mover's Distance. The exact kind of strategy is specified using a SpanFlowStrategy in the SpanFlowSimilarity constructor. The term \"span embeddings\" is a generalization of what is usually called \"document embeddings\" or \"sentence embeddings\". With this strategy we compute one embedding for a span of tokens. We do not specify an explicit strategy for dealing with individual tokens. Instead we provide an encoder that takes a span of tokens and outputs an embedding. To allow various kinds of approaches (such as building span embeddings from token embeddings) the actual encoder is wrapped inside other classes such as SpanEncoder and PartitionEncoder (more on this later). Here is an example of setting up a Waterman-Smith-Beyer alignment that uses cosine similarity over a pretrained GloVe embedding of dimension 50: vectorian.metrics.SpanFlowSimilarity( token_sim=vectorian.metrics( vectorian.embeddings.PretrainedGloVe('6B', ndims=50), vectorian.metrics.CosineSimilarity() ), flow_strategy=vectorian.alignment.WatermanSmithBeyer( gap=vectorian.alignment.ExponentialGapCost(cutoff=5), zero=0.25))","title":"SpanSimilarity"},{"location":"sim_span/#spanflowstrategy","text":"SpanFlowStrategy models a strategy to compute a network flow on the bipartite graph that models a pair of token spans (with tokens as nodes and each part of the graph modelling one span). Specific strategies to do this include classical alignments (e.g. Needleman-Wunsch) and variants of the Word Mover's Distance. The following diagram shows the current implementations for SpanFlowStrategy available in the Vectorian.","title":"SpanFlowStrategy"},{"location":"sim_token/","text":"Token Similarity Overview Modifying Token Similarity","title":"Token Similarity"},{"location":"sim_token/#token-similarity","text":"","title":"Token Similarity"},{"location":"sim_token/#overview","text":"","title":"Overview"},{"location":"sim_token/#modifying-token-similarity","text":"","title":"Modifying Token Similarity"},{"location":"vec_index/","text":"Index What is it for? In order to perform actual searches on Span s, i.e. Document portions, you need to create an Index . The most important method in an Index is the find method, which returns results, as in the following example: my_index.find(\"railway museum\", n=1) Why does it need an Index ? For some kinds of searches, this allows the Vectorian to perform various optimizations under the hood - quite similar to an index in a database system. Certain kinds of Index objects that are expensive to create can also be saved and loaded, but this is beyond the scope of this introduction. Constructing an Index An Index is created from two components, a Partition and a SpanSimilarity : The given Partition indicates the granularity of search and which items should get indexed for searching (see the section on Documents for more details). In short, Partition models how to create Span s from Document s. The SpanSimilarity models the approach taken to compute the similarity of two Span s (e.g. a specific sort of alignment). See the section on Span Similarity for more details. Here is an example ( my_span_sim is an instance of SpanSimilarity ): my_index = session.partition(\"document\").index(my_span_sim, nlp)","title":"Index"},{"location":"vec_index/#index","text":"","title":"Index"},{"location":"vec_index/#what-is-it-for","text":"In order to perform actual searches on Span s, i.e. Document portions, you need to create an Index . The most important method in an Index is the find method, which returns results, as in the following example: my_index.find(\"railway museum\", n=1) Why does it need an Index ? For some kinds of searches, this allows the Vectorian to perform various optimizations under the hood - quite similar to an index in a database system. Certain kinds of Index objects that are expensive to create can also be saved and loaded, but this is beyond the scope of this introduction.","title":"What is it for?"},{"location":"vec_index/#constructing-an-index","text":"An Index is created from two components, a Partition and a SpanSimilarity : The given Partition indicates the granularity of search and which items should get indexed for searching (see the section on Documents for more details). In short, Partition models how to create Span s from Document s. The SpanSimilarity models the approach taken to compute the similarity of two Span s (e.g. a specific sort of alignment). See the section on Span Similarity for more details. Here is an example ( my_span_sim is an instance of SpanSimilarity ): my_index = session.partition(\"document\").index(my_span_sim, nlp)","title":"Constructing an Index"}]}